{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `입력 변수` $x$ `타겟값` $y$가 `선형의 관계`라고 `가정`하고 타겟값을 구하는 알고리즘이다.\n",
    "- 타겟값을 구하기 위해 예측값과 실제값의 `평균제곱오차를 최소화`하는 방향으로 `최소제곱법`을 이용해 최적의 가중치 즉, `회귀계수를 구한다.`\n",
    "- 데이터의 `특성이 많아지면` 최소제곱법의 수식이 복잡해지기 때문에 `확률적 경사하강법`을 적용하여 식을 단순화 시키고 최소제곱법을 적용하는 방법도 있다.\n",
    "- 사이킷런에서 제공하는 LinearRegression 라이브러리는 최소제곱법만을 이용해서 최적의 회귀계수를 구한다.\n",
    "- 확률적 경사하강법을 적용하기 위해선 SGDRegression을 사용하면 된다.\n",
    "- 입력 변수가 하나면 단일 선형 회귀, 여러 개면 다중 선형 회귀라고 부른다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 장단점"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 장점\n",
    "- 학습 속도가 빠르고 예측도 빠르다\n",
    "- 데이터의 크기에 구애받지 않아, 매우 큰 데이터셋과 희소한 데이터셋에도 잘 작동한다.\n",
    "- 예측이 어떻게 만들어지는지 수식을 통해 비교적 쉽게 이해할 수 있다.\n",
    "\n",
    "2. 단점\n",
    "- 입력 변수와 타겟값이 선형이라는 가정을 근거로 기반하고 있기 때문에 실제 모델이 선형적이지 않을 경우, 결과 모델이 데이터에 잘 적합하지 않을 것이다.\n",
    "- 직접 조정할 파라미터가 없기 때문에 가중치 추정 시 폭발적으로 상승할 우려가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 비용 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 최소제곱법\n",
    "- $\\displaystyle argmin\\left(\\sum_{i=1}^{n} ({y_i - w_ix_i})^2 \\right)$\n",
    "- 사이킷런의 LinearRegression 라이브러리에서 최적의 가중치를 찾기 위해 사용되는 비용 함수(=최적화 함수)\n",
    "- 회귀분석에서 평균제곱오차가 최소가 되도록 하는 가중치를 구할 때 사용\n",
    "\n",
    "2. 경사하강법\n",
    "- 회귀분석의 특성이 매우 많아지면 최소제곱법에 적용할 수식의 복잡도가 증가하는 문제를 해결할 때 사용할 수 있는 비용 함수(=최적화 함수)\n",
    "- 사이킷런의 SGDRegressor 라이브러리를 사용하면, 확률적 경사하강법을 적용한 선형회귀를 구현할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 가중치의 추정량을 구할 때, `추정량이 폭발적으로 증가`할 수 있어 이런 경우를 방지하고자 최적화 함수에 `제약`을 거는데, 그 중 하나가 릿지 선형 회귀이다.\n",
    "- 릿지 선형 회귀의 제약 방식은 `기존 선형 회귀의 최적화 함수에 L2 제약식`을 적용한 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 장단점"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 장점\n",
    "- 타겟값과 상관관계가 높은 특성들이 많은 상황에 사용하면 모든 회귀 계수를 줄여주기 때문에 모델의 일반화에 도움이 된다.\n",
    "\n",
    "2. 단점\n",
    "- 특성별 회귀 계수를 통해 모델의 해석이 어느정도 가능했다면, 릿지는 회귀 계수를 임의로 조정하기 때문에 모델 해석이 어려워 진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 최적화 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. RSS + L2 Norm(가중치의 제곱에 조율모수를 곱한 값을 더해줌)\n",
    "- $\\displaystyle argmin\\left(\\sum_{i=1}^{n} ({y_i} - \\hat{y}_i)^2 +  \\lambda\\sum_{j=1}^{p} {w_i}^2\\right)$, $(\\lambda=조율모수(하이퍼 파라미터), )$\n",
    "- 둘의 합을 최소화하는 방향으로 가기 때문에 $\\lambda$가 클수록 RSS가 작아져야하고, 결과적으로 $w$가 작아져야한다.\n",
    "- 따라서 $w$가 작아져서 가중치가 너무 큰 특성의 영향을 줄일 수 있고 모델이 일반화가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ridge와 같은 배경에서 탄생한 제약이 있는 선형 회귀 알고리즘\n",
    "- Ridge는 L2 Norm, Lasso는 L1 Norm을 제약 조건으로 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 장단점"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 장점\n",
    "- 불필요한 변수는 제거되는 경향을 보이기 때문에 중요한 특성을 파악하기 쉽다.\n",
    "\n",
    "2. 단점\n",
    "- 타겟값과 상관관계가 높은 특성들이 많으면, 나머지 상관관계가 낮은 특성들은 모두 사라지게 될 우려가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 최적화 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. RSS + L1 Norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ElasticNet Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 장단점"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 최적화 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 최소제곱법: $x, y$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('cabta')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a063b7be1b1d22be0fca1b26cf478769b2ebdd8896aff1eef11b858cbacfa39b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
